# Testing Guide

## Table of Contents
- [Overview](#overview)
- [Test Categories](#test-categories)
- [Quick Start](#quick-start)
- [Test Execution Commands](#test-execution-commands)
- [Test Coverage](#test-coverage)
- [Integration Testing with Real APIs](#integration-testing-with-real-apis)
- [What Each Test Covers](#what-each-test-covers)
- [Troubleshooting](#troubleshooting)
- [Best Practices](#best-practices)

## Overview

This project uses a comprehensive testing strategy with two main categories:

- **Unit Tests**: Fast, isolated tests with mocked dependencies
- **Integration Tests**: Real API calls and end-to-end workflow testing

**Total Test Coverage**: 87 tests covering all major functionality with high code coverage.

## Test Categories

### üî¨ Unit Tests (Fast & Isolated)
- **Location**: `tests/unit/`
- **Purpose**: Test individual components in isolation
- **Dependencies**: Mocked external services
- **Execution Time**: ~0.2-0.5 seconds
- **Coverage**: 100% for LLM service, 80%+ overall

### üåê Integration Tests (Real & End-to-End)
- **Location**: `tests/integration/`
- **Purpose**: Test complete workflows with real external APIs
- **Dependencies**: Live external services
- **Execution Time**: ~0.5-2 seconds (surprisingly fast!)
- **Coverage**: Critical user journeys and API interactions

## Quick Start

```bash
# Run all tests
python -m pytest

# Run only unit tests (fast)
python -m pytest tests/unit/ -v

# Run only integration tests
python -m pytest tests/integration/ -v

# Run with coverage report
python -m pytest --cov=app --cov-report=term-missing

# Run specific test file
python -m pytest tests/unit/test_ocr_llm_service.py -v
```

## Test Execution Commands

### Basic Test Execution

```bash
# All tests with verbose output
python -m pytest -v

# All tests with short output
python -m pytest

# Stop on first failure
python -m pytest -x

# Run tests in parallel (if you have pytest-xdist)
python -m pytest -n auto
```

### Coverage Analysis

```bash
# Basic coverage report
python -m pytest --cov=app

# Detailed coverage with missing lines
python -m pytest --cov=app --cov-report=term-missing

# HTML coverage report
python -m pytest --cov=app --cov-report=html
# Open htmlcov/index.html in browser

# Coverage for specific module
python -m pytest --cov=app/services/ocr_llm_service --cov-report=term-missing
```

### Test Selection

```bash
# Run only integration tests
python -m pytest -m integration

# Run everything except integration tests
python -m pytest -m "not integration"

# Run specific test class
python -m pytest tests/unit/test_ocr_llm_service.py::TestOCRLLMService

# Run specific test method
python -m pytest tests/unit/test_ocr_llm_service.py::TestOCRLLMService::test_serialization_excludes_none_fields
```

### Debugging Tests

```bash
# Show print statements and detailed output
python -m pytest -s -v

# Show local variables on failure
python -m pytest --tb=long

# Show only short traceback
python -m pytest --tb=short

# Drop into debugger on failure
python -m pytest --pdb
```

## Test Coverage

### Current Coverage Statistics

```
Total Tests: 87
‚îú‚îÄ‚îÄ Unit Tests: 61 tests
‚îÇ   ‚îú‚îÄ‚îÄ OCR LLM Service: 16 tests (100% coverage)
‚îÇ   ‚îú‚îÄ‚îÄ OCR Controller: 15 tests
‚îÇ   ‚îî‚îÄ‚îÄ External OCR Service: 14 tests
‚îî‚îÄ‚îÄ Integration Tests: 26 tests
    ‚îú‚îÄ‚îÄ LLM Integration: 10 tests (Real API calls)
    ‚îú‚îÄ‚îÄ API Endpoints: 17 tests
    ‚îî‚îÄ‚îÄ External OCR: 8 tests
```

### Coverage Analysis by Component

| Component | Unit Tests | Integration Tests | Coverage |
|-----------|------------|-------------------|----------|
| OCR LLM Service | ‚úÖ 16 tests | ‚úÖ 10 tests | 100% |
| OCR Controller | ‚úÖ 15 tests | ‚úÖ Via endpoints | 85% |
| External OCR Service | ‚úÖ 14 tests | ‚úÖ 8 tests | 90% |
| API Routes | ‚úÖ Via controller | ‚úÖ 17 tests | 80% |

## Integration Testing with Real APIs

### Why Real API Testing Matters

Our integration tests make **actual HTTP requests** to live external services:

- **Vision World API**: `http://203.185.131.205/vision-world/process-image`
- **Pathumma Vision OCR LLM**: `http://203.185.131.205/pathumma-vision-ocr/v1/chat/completions`

### What Integration Tests Detect

1. **API Changes**: When external APIs change their response format
2. **Network Issues**: Timeouts, connection problems, rate limits
3. **Serialization Problems**: JSON formatting issues that unit tests miss
4. **Authentication Issues**: API key or authorization problems
5. **Real Performance**: Actual response times and behavior

### Integration Test Features

- **Auto-skip when API unavailable**: Tests gracefully skip if external APIs are down
- **Real image processing**: Uses actual test images from `image/test_image.png`
- **Concurrent testing**: Validates behavior under concurrent load
- **Error handling**: Tests real error scenarios and recovery
- **Clean base64 encoding**: Image encoding utilities with optional logging to avoid clutter

### Image Encoding Utilities

We provide utilities to handle image encoding without cluttering test logs:

```python
from tests.utils.image_utils import encode_test_image, validate_base64, get_test_image_info

# Encode image without logging (clean)
base64_image = encode_test_image(enable_logging=False)

# Encode image with detailed logging (for debugging)
base64_image = encode_test_image(enable_logging=True)

# Validate base64 encoding
is_valid = validate_base64(base64_image)

# Get image information
image_info = get_test_image_info()
```

#### Test Image Encoding Standalone

```bash
# Test image encoding utilities independently
python tests/utils/test_image_encoding.py
```

This script verifies:
- Image file exists and is readable
- Base64 encoding works correctly
- Validation functions work properly
- Logging option works as expected

### API Speed Analysis

**Why are real API calls so fast?**

1. **Optimized test images**: Small file sizes for quick processing
2. **Geographic proximity**: APIs may be geographically close
3. **Server optimizations**: APIs are well-optimized for performance
4. **Fast failure modes**: 400 errors return quickly without processing

```bash
# Test with real image file
python -m pytest tests/integration/test_llm_integration.py::TestLLMIntegration::test_real_llm_api_with_simple_image -v -s
```

## What Each Test Covers

### Unit Tests: `tests/unit/test_ocr_llm_service.py`

| Test Method | Purpose | What It Validates |
|-------------|---------|-------------------|
| `test_process_image_with_llm_success` | End-to-end LLM processing | Complete workflow with mocked API |
| `test_process_image_with_llm_failure` | Error handling | Graceful failure when LLM API fails |
| `test_prepare_multimodal_content` | Content formatting | Proper multimodal message structure |
| `test_call_llm_api_success` | API communication | HTTP request/response handling |
| `test_serialization_excludes_none_fields` | **Critical regression test** | Prevents None field serialization bug |
| `test_multimodal_content_serialization` | JSON formatting | Proper serialization of complex objects |
| `test_health_check_success` | Service monitoring | Health check endpoint functionality |
| `test_custom_model_parameter` | Configuration | Custom model parameter handling |
| `test_timeout_handling` | Error scenarios | Network timeout handling |

### Integration Tests: `tests/integration/test_llm_integration.py`

| Test Method | Purpose | What It Validates |
|-------------|---------|-------------------|
| `test_real_llm_api_connectivity` | API availability | Can connect to live LLM API |
| `test_real_llm_api_with_simple_image` | End-to-end processing | Complete workflow with real API |
| `test_real_serialization_with_api` | **Critical**: Serialization with real API | Ensures exclude_none=True works in production |
| `test_real_api_response_format` | Response validation | API responses match our models |
| `test_real_api_timeout_handling` | Network resilience | Timeout behavior with real network |
| `test_real_concurrent_api_calls` | Load testing | Multiple simultaneous requests |
| `test_real_api_configuration_validation` | Config verification | Service configuration is correct |
| `test_end_to_end_with_real_workflow` | Complete user journey | Full OCR + LLM enhancement workflow |

### Integration Tests: `tests/integration/test_api_endpoints.py`

| Test Category | Purpose | Coverage |
|---------------|---------|----------|
| **Async Processing** | Background task handling | File upload ‚Üí async processing ‚Üí status check |
| **Sync Processing** | Immediate response | File upload ‚Üí immediate OCR result |
| **Error Handling** | Edge cases | Invalid files, missing parameters, rate limits |
| **API Features** | Core functionality | Health checks, task management, CORS |

## Troubleshooting

### Common Issues

#### Integration Tests Fail
```bash
# Check if external APIs are accessible
curl -I http://203.185.131.205/vision-world/process-image
curl -I http://203.185.131.205/pathumma-vision-ocr/v1/chat/completions

# Run without integration tests
python -m pytest -m "not integration"
```

#### Coverage Issues
```bash
# Check if all files are being discovered
python -m pytest --collect-only

# Regenerate coverage report
python -m pytest --cov=app --cov-report=html --cov-fail-under=80
```

#### Slow Test Execution
```bash
# Run only fast unit tests
python -m pytest tests/unit/ -v

# Run with timing information
python -m pytest --durations=10
```

#### Import Errors
```bash
# Verify Python path
python -c "import sys; print('\n'.join(sys.path))"

# Install test dependencies
pip install pytest pytest-cov pytest-asyncio pytest-mock
```

### Debugging Test Failures

#### View Detailed Error Information
```bash
# Show full tracebacks
python -m pytest --tb=long tests/unit/test_ocr_llm_service.py

# Show local variables on failure
python -m pytest --tb=auto --showlocals
```

#### Debug Specific Test
```bash
# Run single test with verbose output
python -m pytest tests/unit/test_ocr_llm_service.py::TestOCRLLMService::test_serialization_excludes_none_fields -v -s

# Drop into debugger on failure
python -m pytest --pdb tests/unit/test_ocr_llm_service.py::TestOCRLLMService::test_serialization_excludes_none_fields
```

## Best Practices

### Running Tests During Development

```bash
# Quick feedback loop (unit tests only)
python -m pytest tests/unit/ -x -v

# Before committing (all tests)
python -m pytest --cov=app --cov-fail-under=80

# Full validation (all tests + coverage)
python -m pytest --cov=app --cov-report=term-missing --cov-fail-under=80
```

### Test Maintenance

1. **Keep tests fast**: Unit tests should run in milliseconds
2. **Mock external dependencies**: Only integration tests should call real APIs
3. **Test edge cases**: Error conditions, invalid inputs, timeouts
4. **Maintain high coverage**: Aim for 80%+ overall, 100% for critical components
5. **Regular integration testing**: Run integration tests before releases

### Adding New Tests

#### For New Features
```python
# 1. Add unit tests first
tests/unit/test_new_feature.py

# 2. Add integration tests if external APIs involved
tests/integration/test_new_feature_integration.py

# 3. Update this documentation
docs/TESTING.md
```

#### Test Naming Convention
```python
# Unit tests: test_[function_name]_[scenario]
def test_process_image_success():
def test_process_image_invalid_format():

# Integration tests: test_real_[feature]_[scenario]
def test_real_api_call_success():
def test_real_api_timeout_handling():
```

### CI/CD Integration

```yaml
# Example GitHub Actions workflow
- name: Run Tests
  run: |
    python -m pytest tests/unit/ --cov=app --cov-fail-under=80
    python -m pytest tests/integration/ --cov=app --cov-append

- name: Generate Coverage Report
  run: |
    python -m pytest --cov=app --cov-report=xml --cov-report=html
```

---

## üìä Testing Metrics

- **Total Tests**: 87
- **Unit Test Coverage**: 100% (LLM Service), 80%+ (Overall)
- **Integration Test Coverage**: All critical user journeys
- **Execution Time**: <2 seconds (including real API calls!)
- **Critical Regression Tests**: Serialization, API format, error handling

**The test suite provides confidence for production deployment while maintaining fast development feedback loops.** 